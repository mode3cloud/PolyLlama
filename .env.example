# Hugging Face variables
HF_TOKEN=
HF_ENDPOINT=http://olah:8090
HF_HUB_ENABLE_HF_TRANSFER=1
HF_HOME=/root/.cache/huggingface


# Debug Options
# OLLAMA_DEBUG=0
# NCCL_DEBUG=INFO


# How CUDA devices are ordered - Ensures consistent device ordering across runs (critical for multi-GPU setups)
CUDA_DEVICE_ORDER=PCI_BUS_ID

# Duration models stay loaded in memory (in seconds) - Longer retention reduces overhead from repeated model loading
OLLAMA_KEEP_ALIVE=-1

# Ollama specific optimization settings (good for quantized GGUF models)
GGML_CUDA_FORCE_MMQ=1
GGML_CUDA_USE_GRAPHS=1
OLLAMA_NOPRUNE=1

# Enable optimized attention mechanism - Improves performance on supported GPUs
OLLAMA_FLASH_ATTENTION=1

# Setting to 0 enables shared memory communication between GPUs Improves performance when GPUs are on the same node
NCCL_SHM_DISABLE=0
# Maximum time (in seconds) NCCL will wait for operations to complete-  Prevents indefinite hangs during collective operations
NCCL_TIMEOUT=90
# Enables asynchronous error handling for NCCL operations in PyTorch - Helps prevent the entire application from hanging when NCCL encounters errors
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
# Forces PyTorch to use blocking wait mode for NCCL operations - Can improve stability at the cost of some performance
TORCH_NCCL_BLOCKING_WAIT=0
# Forces synchronous CUDA kernel launches for debugging, but disabled asynchronous kernel execution which will cost performance
CUDA_LAUNCH_BLOCKING=0


# Disables peer-to-peer (direct GPU-to-GPU) communication Useful when GPUs are on different PCI buses or when P2P isn't working reliably
# NCCL_P2P_DISABLE=1
# Disables InfiniBand transport for multi-node GPU communication Appropriate for single-node setups or when InfiniBand hardware isn't present
# NCCL_IB_DISABLE=1