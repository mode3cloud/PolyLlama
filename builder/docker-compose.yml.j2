name: mode3ai_polyllama

services:
  openwebui:
      build:
        context: ../../stack/open-webui
        dockerfile: Dockerfile
      container_name: openwebui
      ports:
        - "8080:8080"
      volumes:
        - openwebui_data:/app/backend/data

  olah:
    build:
      context: ../../stack/olah
      dockerfile: Dockerfile
    container_name: olah
    volumes:
      - olah_data_repos:/app/repos
      - olah_data_mirrors:/app/mirrors

  llm-proxy:
    build:
      context: ../../stack/llm-proxy
      dockerfile: Dockerfile
    container_name: llm-proxy
    hostname: llm-proxy
    ports:
      - "11435:11435"
    env_file:
      - ../../.env
    environment:
      - OLLAMA_API_BASE=http://router:11434
      - LLM_PROXY_HOST=0.0.0.0
      - LLM_PROXY_PORT=11435
    depends_on:
      - router
  
{% for instance in ollama_instances %}
  polyllama{{ instance.number }}:
    container_name: polyllama{{ instance.number }}
    hostname: polyllama{{ instance.number }}
    build:
      dockerfile: Dockerfile
      context: ../../stack/ollama
    volumes:
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ~/ollama:/root/.ollama
    env_file:
      - ../../.env
    environment:
{% if instance.gpu_indices %}
      - CUDA_VISIBLE_DEVICES={{ instance.gpu_indices }}
{% endif %}
      - OLLAMA_SCHED_SPREAD=1
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    deploy:
      resources:
        limits:
          memory: {{ instance.memory_limit }}g
{% if instance.gpu_indices %}
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
{% endif %}

{% endfor %}

  router:
    container_name: router
    hostname: router
    build:
      context: ../../stack/router
      dockerfile: Dockerfile
{% if dev_mode %}
      target: development
{% else %}
      target: production
{% endif %}
    volumes:
      - ../../stack/router/model_router.lua:/usr/local/openresty/lualib/model_router.lua
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
{% if dev_mode %}
      # Development mode: bind mount the app source
      - ../../stack/router/app:/app
      # Preserve node_modules in the container
      - /app/node_modules
      - /app/.next
{% endif %}
    ports:
      - "11434:11434"
    env_file:
      - ../../.env
    environment:
      - OLLAMA_INSTANCE_COUNT={{ instance_count }}
    depends_on:
{% for instance in ollama_instances %}
      - polyllama{{ instance.number }}
{% endfor %}

volumes:
  openwebui_data:
  olah_data_repos:
  olah_data_mirrors: