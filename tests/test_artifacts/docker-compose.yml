name: mode3ai_polyllama

services:
  openwebui:
      build:
        context: ../../stack/open-webui
        dockerfile: Dockerfile
      container_name: openwebui
      ports:
        - "8080:8080"
      volumes:
        - openwebui_data:/app/backend/data

  olah:
    build:
      context: ../../stack/olah
      dockerfile: Dockerfile
    container_name: olah
    volumes:
      - olah_data_repos:/app/repos
      - olah_data_mirrors:/app/mirrors

  llm-proxy:
    build:
      context: ../../stack/llm-proxy
      dockerfile: Dockerfile
    container_name: llm-proxy
    hostname: llm-proxy
    ports:
      - "8085:8085"
    env_file:
      - ../../.env
    environment:
      - OLLAMA_API_BASE=http://router:11434
      - LLM_PROXY_HOST=0.0.0.0
      - LLM_PROXY_PORT=8085
    depends_on:
      - router
  
  polyllama1:
    container_name: polyllama1
    hostname: polyllama1
    build:
      dockerfile: Dockerfile
      context: ../../stack/ollama
    volumes:
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ~/ollama:/root/.ollama
    env_file:
      - ../../.env
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2
      - OLLAMA_SCHED_SPREAD=1
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    deploy:
      resources:
        limits:
          memory: 96g
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]

  polyllama2:
    container_name: polyllama2
    hostname: polyllama2
    build:
      dockerfile: Dockerfile
      context: ../../stack/ollama
    volumes:
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ~/ollama:/root/.ollama
    env_file:
      - ../../.env
    environment:
      - CUDA_VISIBLE_DEVICES=3
      - OLLAMA_SCHED_SPREAD=1
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    deploy:
      resources:
        limits:
          memory: 32g
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]


  router:
    container_name: router
    hostname: router
    build:
      context: ../../stack/router
      dockerfile: Dockerfile
      target: production
    volumes:
      - ../../stack/router/model_router.lua:/usr/local/openresty/lualib/model_router.lua
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    ports:
      - "11434:11434"
    env_file:
      - ../../.env
    environment:
      - OLLAMA_INSTANCE_COUNT=2
    depends_on:
      - polyllama1
      - polyllama2

volumes:
  openwebui_data:
  olah_data_repos:
  olah_data_mirrors: